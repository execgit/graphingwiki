#!python
# -*- coding: utf-8 -*-
"""
    gwiki-rehash
     - Saves graph data from all the pages of a wiki.

    @copyright: 2006 by Juhani Eronen <exec@iki.fi>
    @license: MIT <http://www.opensource.org/licenses/mit-license.php>

    Permission is hereby granted, free of charge, to any person
    obtaining a copy of this software and associated documentation
    files (the "Software"), to deal in the Software without
    restriction, including without limitation the rights to use, copy,
    modify, merge, publish, distribute, sublicense, and/or sell copies
    of the Software, and to permit persons to whom the Software is
    furnished to do so, subject to the following conditions:

    The above copyright notice and this permission notice shall be
    included in all copies or substantial portions of the Software.

    THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
    EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
    MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
    NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT
    HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY,
    WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
    OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
    DEALINGS IN THE SOFTWARE.

"""

import sys
import os
sys.path.append(os.path.join(os.path.split(os.path.abspath(__file__))[0], '..', 'lib', 'python%d.%d' % sys.version_info[:2], 'site-packages'))

from codecs import getencoder
from optparse import OptionParser

from MoinMoin.wikiutil import quoteWikinameFS, unquoteWikiname, importPlugin
from MoinMoin.request.request_cli import Request as RequestCLI
from MoinMoin.Page import Page
from MoinMoin import config

from graphingwiki.editing import underlay_to_pages

usage = "usage: %prog <path-to-wiki> [pagename]\n"
parser = OptionParser(usage=usage)

(options, args) = parser.parse_args()

# Encoder from unicode to charset selected in config
encoder = getencoder(config.charset)
def _e(str):
    return encoder(str, 'replace')[0]

pages = []

class UserInputException(Exception):
    pass

try:
    wikipath = args[0]
    configdir = os.path.abspath(os.path.join(wikipath, 'config'))

    sys.path.insert(0, configdir)
    from wikiconfig import Config
    datadir = Config.data_dir
    pagesdir = os.path.join(datadir, 'pages')
    if len(args) > 1:
        pagename = unicode(args[1], sys.getfilesystemencoding())
        pagefilename = quoteWikinameFS(pagename)
        pagedir = os.path.join(pagesdir, pagefilename)
        if not os.path.isdir(pagedir):
            raise UserInputException
        pages = [pagename]
except UserInputException:
    parser.print_help()
    sys.exit(2)

if not pages:
    pass
    for file in os.listdir(datadir):
        if file == 'graphdata.shelve':
            sys.stderr.write('Removed graphdata (%s)\n' % file)
            os.unlink(os.path.join(datadir, file))
else:
    for file in os.listdir(datadir):
        if file.startswith('read_lock') or file.startswith('write_lock'):
            sys.stderr.write("Found lockfile (%s), \n" % file + \
                             "database may be corrupted. \n" + \
                             "Rehash entire db or remove lock " + \
                             "and rehash the crashed pages.\n")
            sys.exit(1)

os.chdir(pagesdir)

if not pages:
    # List existing pages
    for dir in [x for x in os.listdir('.') if os.path.isdir(x)]:
        if 'current' in os.listdir(dir):
            pages.append(unquoteWikiname(dir))

total = len(pages)
padding = len(str(total))
count = 1

for pagename in pages:
    if pagename.endswith('/MoinEditorBackup'):
        continue
    print "Rehashing %s (%*d/%*d)" % (_e(pagename), padding, count, padding, count)
    count += 1
    page_enc = _e(pagename)
    # Make a new request for the page, get raw text
    req = RequestCLI(pagename=page_enc)
    pagedir = os.path.join(req.cfg.data_dir, 'pages')

    picklefile = os.path.join(pagedir, 
                              quoteWikinameFS(pagename), 
                              'graphdata.pickle')

    if os.path.isfile(picklefile):
        os.unlink(picklefile)

    p = Page(req, pagename)

    pagepath = underlay_to_pages(req, p)

    text = p.get_raw_body()
    
    # Apply the graphsaver-action to the page
    graphsaver = importPlugin(req.cfg, 'action', 'savegraphdata')
    graphsaver(pagename, req, text, pagepath, p)
