--- ./MoinMoin/Page.py.orig	2015-04-21 06:08:39.249947809 +0300
+++ ./MoinMoin/Page.py	2015-12-07 00:48:25.893339512 +0200
@@ -38,6 +38,7 @@ import os, re, codecs
 
 from MoinMoin import log
 logging = log.getLogger(__name__)
+SAVED_LAZY = 1
 
 from MoinMoin import config, caching, user, util, wikiutil
 from MoinMoin.logfile import eventlog
@@ -108,8 +109,7 @@ class ItemCache:
             (for 'meta') or the complete cache ('pagelists').
             @param request: the request object
         """
-        from MoinMoin.logfile import editlog
-        elog = editlog.EditLog(request)
+        elog = request.editlog
         old_pos = self.log_pos
         new_pos, items = elog.news(old_pos)
         if items:
@@ -218,6 +218,26 @@ class Page(object):
                     # return empty text (note that we never store empty pages,
                     # so this is detectable and also safe when passed to a
                     # function expecting a string)
+                    if self.exists():
+                        from graphingwiki.editing import (metatable_parseargs, get_metas, 
+                                                          replace_metas)
+                        _, metakeys, _ = metatable_parseargs(self.request, self.page_name,
+                                                             get_all_keys=True)
+                        pagemeta = get_metas(self.request, self.page_name, metakeys, 
+                                             indirection=False, checkAccess=False)
+                        template = pagemeta.get('gwikitemplate', [''])[0]
+                        temp_body = ''
+                        temp_meta = dict()
+                        if template:
+                            template_page = wikiutil.unquoteWikiname(template)
+                            if self.request.user.may.read(template_page):
+                                temp_body = Page(self.request, template_page).get_raw_body()
+                                _, metakeys, _ = metatable_parseargs(self.request, template_page,
+                                                                     get_all_keys=True)
+                                temp_meta = get_metas(self.request, template_page, metakeys)
+                            pagemeta.pop('gwikitemplate')
+                        text = replace_metas(self.request, temp_body, temp_meta, pagemeta)
+                        return text
                     return u""
                 else:
                     raise
@@ -525,7 +545,7 @@ class Page(object):
             entry = request.cfg.cache.meta.getItem(request, cache_name, cache_key)
         else:
             entry = None
-        if entry is None:
+        if entry is None and not self.is_lazy():
             from MoinMoin.logfile import editlog
             wanted_rev = "%08d" % self.get_real_rev()
             edit_log = editlog.EditLog(request, rootpagename=self.page_name)
@@ -597,9 +617,15 @@ class Page(object):
         """ Can this page be changed?
 
         @rtype: bool
-        @return: true, if this page is writable or does not exist
+        @return: true, if this page is in the backend, writable or does not exist
         """
-        return os.access(self._text_filename(), os.W_OK) or not self.exists()
+        if self._in_backend():
+            return True
+        if os.access(self._text_filename(), os.W_OK):
+            return True
+        if not self.exists():
+            return True
+        return False
 
     def isUnderlayPage(self, includeDeleted=True):
         """ Does this page live in the underlay dir?
@@ -626,7 +652,15 @@ class Page(object):
         """
         return self.exists(domain='standard', includeDeleted=includeDeleted)
 
-    def exists(self, rev=0, domain=None, includeDeleted=False):
+    def _in_backend(self):
+        if self.page_name in self.request.graphdata:
+            return self.request.graphdata.is_saved(self.page_name)
+        return 0
+
+    def is_lazy(self, includeBackend=True):
+        return self._in_backend() == SAVED_LAZY
+
+    def exists(self, rev=0, domain=None, includeDeleted=False, includeBackend=True):
         """ Does this page exist?
 
         This is the lower level method for checking page existence. Use
@@ -644,6 +678,7 @@ class Page(object):
         if domain == 'underlay' and not self.request.cfg.data_underlay_dir:
             return False
 
+
         if includeDeleted:
             # Look for page directory, ignore page state
             if domain is None:
@@ -656,6 +691,12 @@ class Page(object):
                     return True
             return False
         else:
+            # If it's in the backend, it exists
+            if includeBackend and self._in_backend():
+                return True
+            elif includeBackend:
+                return False
+
             # Look for non-deleted pages only, using get_rev
             if not rev and self.rev:
                 rev = self.rev
@@ -1090,7 +1131,11 @@ class Page(object):
             # to ensure cacheability where supported. Because we are sending
             # RAW (file) content, the file mtime is correct as Last-Modified header.
             request.status_code = 200
-            request.last_modified = os.path.getmtime(self._text_filename())
+            try:
+                request.last_modified = os.path.getmtime(self._text_filename())
+            except OSError:
+                # lazy pages, errno 2
+                pass
             text = self.encodeTextMimeType(self.body)
             #request.headers['Content-Length'] = len(text)  # XXX WRONG! text is unicode obj, but we send utf-8!
             if content_disposition:
@@ -1353,10 +1398,11 @@ class Page(object):
 
         # cache the pagelinks
         if do_cache and self.default_formatter and page_exists:
-            cache = caching.CacheEntry(request, self, 'pagelinks', scope='item', use_pickle=True)
-            if cache.needsUpdate(self._text_filename()):
-                links = self.formatter.pagelinks
-                cache.update(links)
+            if not self.is_lazy():
+                cache = caching.CacheEntry(request, self, 'pagelinks', scope='item', use_pickle=True)
+                if cache.needsUpdate(self._text_filename()):
+                    links = self.formatter.pagelinks
+                    cache.update(links)
 
         # restore old formatter (hopefully we dont throw any exception that is catched again)
         if old_formatter is no_formatter:
@@ -1418,7 +1464,7 @@ class Page(object):
             Parser = wikiutil.searchAndImportPlugin(request.cfg, "parser", "plain")
         parser = Parser(body, request, format_args=format_args, **kw)
 
-        if not (do_cache and self.canUseCache(Parser)):
+        if not (do_cache and self.canUseCache(Parser)) or self.is_lazy():
             self.format(parser)
         else:
             try:
@@ -1495,8 +1541,9 @@ class Page(object):
         src = formatter.assemble_code(text)
         code = compile(src.encode(config.charset),
                        self.page_name.encode(config.charset), 'exec')
-        cache = caching.CacheEntry(request, self, self.getFormatterName(), scope='item')
-        cache.update(marshal.dumps(code))
+        if not self.is_lazy():
+            cache = caching.CacheEntry(request, self, self.getFormatterName(), scope='item')
+            cache.update(marshal.dumps(code))
         return code
 
     def _specialPageText(self, request, special_type):
@@ -1601,7 +1648,7 @@ class Page(object):
         @rtype: list
         @return: page names this page links to
         """
-        if self.exists():
+        if self.exists() and not self.is_lazy():
             cache = caching.CacheEntry(request, self, 'pagelinks', scope='item', do_locking=False, use_pickle=True)
             if cache.needsUpdate(self._text_filename()):
                 links = self.parsePageLinks(request)
@@ -1612,6 +1659,8 @@ class Page(object):
                 except caching.CacheError:
                     links = self.parsePageLinks(request)
                     cache.update(links)
+        elif self.is_lazy():
+            links = self.parsePageLinks(request)
         else:
             links = []
         return links
@@ -1731,13 +1780,17 @@ class Page(object):
         """
         from MoinMoin import security
         if self.exists() and self.rev == 0:
-            return self.pi['acl']
+            page = self.request.graphdata.getpage(self.page_name)
+            acl = page.get('acl', [])
+            return security.AccessControlList(self.request.cfg, acl)
         try:
             lastRevision = self.getRevList()[0]
         except IndexError:
             return security.AccessControlList(self.request.cfg)
         if self.rev == lastRevision:
-            return self.pi['acl']
+            page = self.request.graphdata.getpage(self.page_name)
+            acl = page.get('acl', [])
+            return security.AccessControlList(self.request.cfg, acl)
 
         return Page(self.request, self.page_name, rev=lastRevision).parseACL()
 
@@ -1826,7 +1879,7 @@ class RootPage(Page):
 
         return underlay, path
 
-    def getPageList(self, user=None, exists=1, filter=None, include_underlay=True, return_objects=False):
+    def getPageList(self, user=None, exists=1, filter=None, include_underlay=True, return_objects=False, include_backend=True):
         """ List user readable pages under current page
 
         Currently only request.rootpage is used to list pages, but if we
@@ -1895,7 +1948,7 @@ class RootPage(Page):
                     continue
 
                 # Filter deleted pages
-                if exists and not page.exists():
+                if exists and not page.exists(includeBackend=include_backend):
                     continue
 
                 # Filter out page user may not read.
